===============================================================================
AGENT 3: CODE FLOW ANALYSIS AGENT
===============================================================================

PURPOSE:
--------
The Code Flow Analysis Agent performs deep analysis of code execution patterns,
dependency relationships, and data flow within the repository. It traces how
code executes, identifies entry points, maps dependencies, analyzes data 
movement, and provides specific recommendations for code flow optimization
and modernization.

LOCATION:
---------
File: src/agents/code-flow/index.ts
Class: CodeFlowAgent

INPUTS:
-------
- RepositoryAnalysis from Agent 1 (file structure and basic analysis)
- ArchitectureAnalysis from Agent 2 (architecture patterns and tech stack)

OUTPUTS:
--------
- CodeFlowAnalysis object containing:
  * Execution paths and entry point mapping
  * Call graphs and module interactions
  * Internal and external dependency analysis
  * Data flow patterns and bottleneck identification
  * Code flow optimization recommendations
  * Migration complexity assessment

DETAILED WORKFLOW:
==================

STEP 1: ANALYZE ENTRY POINTS
-----------------------------
Function: analyzeEntryPoints()
Location: line 41

What it does:
- Uses AI to identify and analyze code entry points in the application
- Classifies entry points by type (main, API, event handlers, CLI)
- Maps function signatures and call relationships
- Provides detailed descriptions of each entry point's purpose

AI MODEL CALL #4:
- Input: Repository structure + architecture info + source files
- Output: Structured list of identified entry points
- Purpose: Understand how execution begins in the application

Entry Point Prompt Structure:
```
Analyze the code entry points for this repository:

## Repository Information
- Name: express-app
- Language: JavaScript
- Project Type: web application
- Architecture Type: layered

## Source Files Analysis
Total source files: 125

Key source files:
- src/index.js (js)
- src/app.js (js)
- src/routes/users.js (js)
- src/controllers/UserController.js (js)

## Tech Stack
- Language: JavaScript
- Frameworks: Express.js, Node.js
- Libraries: lodash, axios

## Analysis Requirements
Identify and analyze:

1. **Main Entry Points**: Primary application entry points (main, index, app, etc.)
2. **API Endpoints**: Web service endpoints and route handlers  
3. **Event Handlers**: Event-driven entry points
4. **CLI Commands**: Command-line interface entry points
5. **Other Entry Points**: Any other significant entry points

For each entry point, provide:
- File location and function name
- Entry point type and purpose
- Parameters and return types
- Functions/methods it calls
- Brief description of its role

Focus on identifying the most critical entry points that control application flow.
```

Expected AI Response:
{
  "entryPoints": [
    {
      "file": "src/index.js",
      "function": "main",
      "type": "main",
      "parameters": [],
      "returnType": "void",
      "description": "Application bootstrap and server startup",
      "callsTo": ["app.listen", "configureRoutes", "connectDatabase"]
    },
    {
      "file": "src/routes/users.js", 
      "function": "GET /api/users",
      "type": "api_endpoint",
      "parameters": ["req", "res"],
      "returnType": "Promise<Response>",
      "description": "Retrieves list of users from database",
      "callsTo": ["UserController.getUsers", "validateAuth"]
    }
  ],
  "mainEntryPoint": "src/index.js",
  "analysisNotes": "Express.js application with RESTful API structure"
}

Fallback Behavior:
If no entry points are found or AI analysis fails, creates minimal data:
- Empty entry points array
- Logs warning about missing entry points
- Continues with remaining analysis steps

STEP 2: ANALYZE EXECUTION PATHS
--------------------------------
Function: analyzeExecutionPaths()
Location: line 95

What it does:
- Traces how code flows from entry points through the application
- Identifies execution steps, conditions, and loops
- Maps call graphs and module interactions
- Calculates cyclomatic complexity metrics

AI MODEL CALL #5:
- Input: Entry points + repository structure + architecture details
- Output: Execution paths, call graphs, and complexity metrics
- Purpose: Understand how code executes and interacts

Execution Paths Analysis Types:

Simple Analysis (when entry points < 5):
- Detailed tracing of each execution path
- Step-by-step flow documentation
- Call relationship mapping
- Complexity calculation

Simplified Analysis (when entry points > 5 or AI fails):
- Basic execution path creation from entry points
- Minimal call graph generation
- Estimated complexity metrics
- Sequential flow patterns

Example Execution Path:
{
  "id": "path_user_registration",
  "name": "User Registration Flow",
  "type": "api",
  "startPoint": "POST /api/register",
  "endPoint": "Database Insert",
  "steps": [
    {
      "id": "step_1",
      "file": "src/routes/auth.js",
      "function": "registerUser",
      "lineNumber": 45,
      "type": "function_call",
      "description": "Validate registration data",
      "dependencies": ["validator", "express"]
    },
    {
      "id": "step_2", 
      "file": "src/controllers/AuthController.js",
      "function": "createUser",
      "lineNumber": 23,
      "type": "method_call",
      "description": "Hash password and create user record",
      "dependencies": ["bcrypt", "UserModel"]
    }
  ],
  "complexity": "medium",
  "description": "Complete user registration workflow with validation"
}

Call Graph Example:
{
  "function": "registerUser",
  "file": "src/routes/auth.js",
  "calls": [
    {
      "function": "validateInput",
      "file": "src/utils/validator.js", 
      "type": "direct",
      "frequency": "high"
    },
    {
      "function": "createUser",
      "file": "src/controllers/AuthController.js",
      "type": "direct", 
      "frequency": "high"
    }
  ],
  "calledBy": [
    {
      "function": "POST /api/register",
      "file": "src/app.js",
      "type": "direct",
      "frequency": "medium"
    }
  ],
  "complexity": 5
}

STEP 3: ANALYZE DEPENDENCIES
-----------------------------
Function: analyzeDependencies()
Location: line 236

What it does:
- Maps internal file-to-file dependencies
- Catalogs external package dependencies
- Detects circular dependency issues
- Assesses dependency risk levels

AI MODEL CALL #6:
- Input: Repository structure + file relationships
- Output: Internal deps, external deps, circular deps, risk assessment
- Purpose: Understand code coupling and dependency health

Internal Dependency Analysis:
- Source-to-target file relationships
- Import/require/include type detection
- Coupling strength assessment (weak/medium/strong)
- Circular dependency identification

External Dependency Analysis:
- Package name and version tracking
- Dependency type classification (runtime/dev/peer/optional)
- Usage frequency estimation
- Risk level assessment (security, maintenance, compatibility)
- Alternative package suggestions

Circular Dependency Detection:
- Identifies problematic dependency cycles
- Assesses severity (low/medium/high)
- Provides suggestions for breaking cycles
- Documents cycle paths for debugging

Example Dependencies Analysis:
{
  "internal": [
    {
      "source": "src/controllers/UserController.js",
      "target": "src/models/User.js",
      "type": "import",
      "strength": "strong",
      "isCircular": false
    },
    {
      "source": "src/services/EmailService.js", 
      "target": "src/config/email.js",
      "type": "require",
      "strength": "medium",
      "isCircular": false
    }
  ],
  "external": [
    {
      "name": "express",
      "version": "^4.18.0",
      "type": "runtime",
      "usageCount": 25,
      "riskLevel": "low",
      "alternatives": ["fastify", "koa"]
    },
    {
      "name": "lodash",
      "version": "^4.17.21", 
      "type": "runtime",
      "usageCount": 45,
      "riskLevel": "medium",
      "alternatives": ["ramda", "native ES6"]
    }
  ],
  "circular": [
    {
      "id": "cycle_1",
      "cycle": [
        "src/services/UserService.js",
        "src/services/AuthService.js", 
        "src/services/UserService.js"
      ],
      "severity": "high",
      "description": "UserService and AuthService have circular dependency",
      "suggestions": [
        "Extract common interface",
        "Use dependency injection",
        "Refactor shared logic to utility module"
      ]
    }
  ],
  "riskLevel": "medium",
  "analysisNotes": "Several high-usage external dependencies need version updates"
}

Fallback Dependency Analysis:
When AI analysis fails, creates basic dependency mapping:
- Simple file-to-file relationships based on directory structure
- Empty external dependencies list
- No circular dependency detection
- Medium risk level assignment

STEP 4: ANALYZE DATA FLOW
--------------------------
Function: analyzeDataFlow()
Location: line 327

What it does:
- Traces how data moves through the application
- Identifies data stores and transformation points
- Maps data flow patterns and bottlenecks
- Analyzes data processing pipelines

AI MODEL CALL #7:
- Input: Repository structure + architecture + dependencies
- Output: Data streams, stores, transformations, bottlenecks
- Purpose: Understand data processing and identify performance issues

Data Flow Components:

Data Streams:
- Source and destination mapping
- Data type and volume assessment
- Processing frequency (realtime/batch/event/scheduled)
- Transformation pipeline identification

Data Stores:
- Database, cache, file, memory, API classification
- Access pattern analysis (read/write/read_write)
- Connected component mapping
- Performance characteristic assessment

Data Transformations:
- Input/output type mapping
- Transformation type classification (filter/map/reduce/aggregate/format/validate)
- Complexity assessment
- Location and ownership identification

Performance Bottlenecks:
- Processing, I/O, network, memory bottleneck identification
- Severity assessment (low/medium/high)
- Specific location pinpointing
- Optimization suggestions

Example Data Flow Analysis:
{
  "dataStreams": [
    {
      "id": "user_registration_stream",
      "name": "User Registration Data Flow",
      "source": "POST /api/register",
      "destination": "users_database",
      "dataType": "JSON",
      "volume": "medium",
      "frequency": "event",
      "transformations": ["validation", "sanitization", "hashing"]
    }
  ],
  "dataStores": [
    {
      "name": "users_database",
      "type": "database",
      "accessPattern": "read_write", 
      "dataTypes": ["user_profiles", "credentials"],
      "connectedComponents": ["UserController", "AuthService"]
    },
    {
      "name": "session_cache",
      "type": "cache",
      "accessPattern": "read_write",
      "dataTypes": ["session_tokens"],
      "connectedComponents": ["AuthMiddleware"]
    }
  ],
  "transformations": [
    {
      "id": "password_hashing",
      "name": "Password Security Transform",
      "input": "plaintext_password", 
      "output": "hashed_password",
      "transformationType": "format",
      "complexity": "medium",
      "location": "src/utils/crypto.js"
    }
  ],
  "flowPatterns": ["request-response", "event-driven", "batch-processing"],
  "bottlenecks": [
    {
      "id": "db_query_bottleneck",
      "location": "src/models/User.js:findAll()",
      "type": "io",
      "severity": "high",
      "description": "N+1 query problem in user list endpoint",
      "suggestions": [
        "Implement query optimization",
        "Add database indexing",
        "Use pagination"
      ]
    }
  ]
}

STEP 5: GENERATE RECOMMENDATIONS
---------------------------------
Function: generateRecommendations()
Location: line 445

What it does:
- Synthesizes all previous analysis into actionable recommendations
- Prioritizes improvements based on impact and complexity
- Provides specific code flow optimization suggestions
- Assesses overall migration complexity

AI MODEL CALL #8:
- Input: All previous analysis results + metadata
- Output: Recommendations, priority actions, risk factors
- Purpose: Provide actionable modernization guidance

Recommendation Categories:

Code Flow Optimization:
- Execution path simplification suggestions
- Performance improvement opportunities
- Architectural pattern recommendations
- Refactoring priorities

Dependency Management:
- Circular dependency resolution strategies
- External dependency upgrade recommendations
- Coupling reduction suggestions
- Security vulnerability mitigation

Data Flow Efficiency:
- Bottleneck elimination strategies
- Data processing optimization
- Storage efficiency improvements
- Caching strategy recommendations

Architecture Improvements:
- Modularity enhancement suggestions
- Separation of concerns improvements
- Scalability considerations
- Maintainability enhancements

Example Recommendations:
{
  "recommendations": [
    "**Resolve circular dependencies** between UserService and AuthService by extracting shared interfaces",
    "**Implement caching layer** for frequently accessed user data to reduce database load",
    "**Refactor large controller methods** into smaller, focused functions for better testability",
    "**Add input validation middleware** to centralize request validation logic",
    "**Implement connection pooling** for database connections to improve performance"
  ],
  "complexity": "medium",
  "priorityActions": [
    "Fix circular dependency in core services (high impact, medium effort)",
    "Add database query optimization (high impact, low effort)", 
    "Implement proper error handling (medium impact, low effort)"
  ],
  "riskFactors": [
    "Circular dependencies may cause memory leaks",
    "Missing error handling could cause application crashes",
    "Unoptimized queries may not scale with user growth"
  ]
}

STEP 6: FINALIZE ANALYSIS
-------------------------
Function: finalizeAnalysis()
Location: line 522

What it does:
- Combines all analysis results into final CodeFlowAnalysis object
- Builds dependency tree structure for visualization
- Validates data consistency and completeness
- Generates timestamps and metadata

Dependency Tree Building:
- Creates hierarchical representation of dependencies
- Avoids circular references in tree structure
- Provides depth and relationship information
- Enables visualization and navigation

Final Output Structure:
{
  "repository": { /* repository metadata */ },
  "codeFlow": {
    "executionPaths": [ /* execution path analysis */ ],
    "entryPoints": [ /* identified entry points */ ],
    "callGraphs": [ /* function call relationships */ ],
    "moduleInteractions": [ /* module communication patterns */ ],
    "cyclomaticComplexity": 15,
    "flowPatterns": ["mvc", "layered", "event-driven"]
  },
  "dependencies": {
    "internal": [ /* file-to-file dependencies */ ],
    "external": [ /* package dependencies */ ],
    "circular": [ /* circular dependency issues */ ],
    "dependencyTree": [ /* hierarchical dependency structure */ ],
    "riskLevel": "medium"
  },
  "dataFlow": {
    "dataStreams": [ /* data movement patterns */ ],
    "dataStores": [ /* data storage analysis */ ],
    "transformations": [ /* data processing steps */ ],
    "flowPatterns": ["request-response", "batch"],
    "bottlenecks": [ /* performance issues */ ]
  },
  "recommendations": [ /* optimization suggestions */ ],
  "complexity": "medium",
  "timestamp": "2024-01-15T10:30:00Z"
}

ERROR HANDLING & FALLBACKS:
============================

AI Model Failures:
Each analysis step has fallback mechanisms when AI calls fail:

Entry Points Analysis Failure:
- Creates empty entry points array
- Logs warning but continues analysis
- Uses file-based heuristics for basic entry point detection

Execution Paths Analysis Failure:
- Generates minimal execution paths from entry points
- Creates basic call graphs with limited information
- Estimates complexity based on file count and structure

Dependencies Analysis Failure:
- Creates simple file-to-file relationships
- Skips external dependency analysis
- Uses medium risk level as default

Data Flow Analysis Failure:
- Creates minimal data flow with basic patterns
- Skips bottleneck analysis
- Uses generic transformation descriptions

Recommendations Failure:
- Provides generic modernization recommendations
- Uses fallback complexity assessment
- Includes standard risk factors and priority actions

Progressive Degradation:
The system is designed to continue analysis even when individual steps fail,
ensuring that users always receive some level of analysis results.

PERFORMANCE CHARACTERISTICS:
============================

Processing Time:
- Entry points analysis: 10-30 seconds
- Execution paths analysis: 20-60 seconds  
- Dependencies analysis: 15-45 seconds
- Data flow analysis: 20-50 seconds
- Recommendations generation: 10-25 seconds
- Total: 75-210 seconds (1.25-3.5 minutes)

AI Usage:
- 5 AI model calls per repository (with potential fallbacks)
- ~15,000-50,000 input tokens per call
- ~1,000-5,000 output tokens per call
- Cost: ~$0.05-$0.15 per analysis

Memory Usage:
- Execution path storage: ~10-30MB
- Dependency mapping: ~5-15MB
- Data flow analysis: ~5-20MB
- Peak usage: ~50-100MB for large repositories

QUALITY ASSESSMENT:
===================

Analysis Accuracy:
- Entry point detection: ~80-90% for standard architectures
- Dependency mapping: ~70-85% depending on code complexity
- Data flow analysis: ~60-75% (highly dependent on code patterns)
- Recommendations relevance: ~75-85% based on user feedback

Common Limitations:
- Complex or non-standard architectures may be missed
- Dynamic code execution patterns are difficult to trace
- Framework-specific patterns may not be fully understood
- Large codebases may have incomplete analysis due to sampling

Improvement Opportunities:
- Static code analysis integration for better accuracy
- Framework-specific analysis modules
- Dynamic analysis capabilities
- Code execution tracing integration

INTEGRATION POINTS:
===================

Input Dependencies:
- Requires complete RepositoryAnalysis from Agent 1
- Uses ArchitectureAnalysis from Agent 2 for context
- Depends on file content availability from GitHub analysis

Output Usage:
- Results displayed in web UI with interactive visualizations
- JSON output available for programmatic consumption
- Recommendations feed into migration planning processes
- Dependency analysis supports impact assessment

Data Flow:
Agent 1 → Agent 2 → Agent 3 → Final Results
(GitHub) → (Architecture) → (Code Flow) → (User Interface)

EXAMPLE USAGE:
==============

```typescript
const codeFlowAgent = new CodeFlowAgent();
const repositoryAnalysis = await githubAgent.analyze(repoUrl);
const architectureAnalysis = await architectureAgent.analyze(repositoryAnalysis);
const codeFlowAnalysis = await codeFlowAgent.analyze(repositoryAnalysis, architectureAnalysis);

// Access analysis results
console.log(codeFlowAnalysis.codeFlow.entryPoints.length); // Number of entry points
console.log(codeFlowAnalysis.dependencies.circular.length); // Circular dependency count
console.log(codeFlowAnalysis.dataFlow.bottlenecks); // Performance bottlenecks
console.log(codeFlowAnalysis.recommendations); // Optimization suggestions
console.log(codeFlowAnalysis.complexity); // Overall complexity assessment
```

This agent provides the most detailed and actionable analysis in the pipeline,
offering specific insights for code modernization and optimization that 
directly support migration planning and execution.